{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2025 Semester 1\n",
    "\n",
    "## Assignment 1: Scam detection with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Student ID(s):**     `1462474`\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, GRAPHS, AND FIGURES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).** Results, figures, etc. which appear in this file but are NOT included in your report will not be marked.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised model training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:02.915312Z",
     "start_time": "2025-04-09T11:47:02.911594Z"
    }
   },
   "source": [
    "## Import necessary libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 295
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Read in supervised training dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:02.967763Z",
     "start_time": "2025-04-09T11:47:02.957020Z"
    }
   },
   "cell_type": "code",
   "source": "sms_df = pd.read_csv('sms_supervised_train.csv')",
   "outputs": [],
   "execution_count": 296
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reformat text to help with tokenising"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.012230Z",
     "start_time": "2025-04-09T11:47:03.006359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensures data types are as intended and no nulls\n",
    "def preprocess_data(df):\n",
    "    df['textPreprocessed'] = df['textPreprocessed'].astype(str)\n",
    "    df = df.dropna(subset=['textPreprocessed'])\n",
    "    df['class'] = df['class'].astype(int)\n",
    "    return df\n",
    "\n",
    "sms_df = preprocess_data(sms_df)"
   ],
   "outputs": [],
   "execution_count": 297
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build vocabulary list"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.024365Z",
     "start_time": "2025-04-09T11:47:03.016222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define vocab list (set for build efficiency)\n",
    "vocab_set = set()\n",
    "\n",
    "# For each row in the dataset, split text into words and add them to the vocab set\n",
    "# Convert to a list at the end\n",
    "def create_vocab_list(df):\n",
    "    vocab_set = set()\n",
    "    for text in df['textPreprocessed']:\n",
    "        words = text.split()\n",
    "        vocab_set.update(words)\n",
    "    return list(vocab_set)\n",
    "\n",
    "# Create vocab list\n",
    "vocab_list = create_vocab_list(sms_df)\n"
   ],
   "outputs": [],
   "execution_count": 298
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build count matrix"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.100800Z",
     "start_time": "2025-04-09T11:47:03.065304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creates a count matrix where each row represents a text instance\n",
    "# Columns represent the words in the vocab list.\n",
    "# Returns the matrix and a dictionary mapping words to their index in the vocab list\n",
    "def build_count_matrix(df, vocab_list):\n",
    "    # Initialise empty count matrix\n",
    "    count_matrix = np.zeros((df.shape[0], len(vocab_list)))\n",
    "    # Create dictionary mapping words to their index in the vocab list\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab_list)}\n",
    "    for index, text in df['textPreprocessed'].items():\n",
    "        for word in text.split():\n",
    "            # If the word is in the vocab list, increment its count in the matrix\n",
    "            if word in vocab_dict:\n",
    "                word_index = vocab_dict.get(word)\n",
    "                count_matrix[index][word_index] += 1\n",
    "    return count_matrix, vocab_dict\n",
    "\n",
    "# Build count matrix\n",
    "count_matrix,vocab_dict = build_count_matrix(sms_df, vocab_list)"
   ],
   "outputs": [],
   "execution_count": 299
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute the prior probability of each class:\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.147813Z",
     "start_time": "2025-04-09T11:47:03.140191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the prior probability of each class given the dataframe\n",
    "def calculate_priors(df):\n",
    "    n_rows = df.shape[0]\n",
    "    # Count the number of instances in each class\n",
    "    n_c0 = df[df['class'] == 0].shape[0]\n",
    "    n_c1 = df[df['class'] == 1].shape[0]\n",
    "    # Calculate the prior probability of each class\n",
    "    p_c0 = n_c0 / n_rows\n",
    "    p_c1 = n_c1 / n_rows\n",
    "    return [p_c0, p_c1]\n",
    "\n",
    "# Calculate priors\n",
    "priors = calculate_priors(sms_df)\n",
    "p_c0 = priors[0]\n",
    "p_c1 = priors[1]\n",
    "\n",
    "# Answer to question 1 in our report\n",
    "print(f\"Our two priors are p_c0 = {p_c0}\\nand p_c1 = {p_c1} \")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our two priors are p_c0 = 0.8\n",
      "and p_c1 = 0.2 \n"
     ]
    }
   ],
   "execution_count": 300
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Find the probability of each word appearing in a message from each class (likelihood)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.207455Z",
     "start_time": "2025-04-09T11:47:03.190033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First we need to find the number of times each word appears in each class\n",
    "\n",
    "''' Counts the number of times each word appears in a given class\n",
    "    returns an list of counts '''\n",
    "def count_words_in_class(class_num, train_df, count_matrix):\n",
    "    # Find the indexes of the rows in the count matrix that belong to the given class\n",
    "    index_list = np.where(train_df['class'] == class_num)[0]\n",
    "    # Get the count matrix for the given class (only shows rows where we classify as class_num)\n",
    "    class_count_matrix = count_matrix[index_list]\n",
    "    # Sums the word counts across all text instances for the class\n",
    "    count_list = class_count_matrix.sum(axis=0)\n",
    "    return count_list\n",
    "\n",
    "# Class 0 count\n",
    "count_c0 = count_words_in_class(0, sms_df, count_matrix)\n",
    "\n",
    "# Class 1 count\n",
    "count_c1 = count_words_in_class(1, sms_df, count_matrix)\n"
   ],
   "outputs": [],
   "execution_count": 301
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.265848Z",
     "start_time": "2025-04-09T11:47:03.255059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We will use laplace smoothing to ensure every event has a non-zero probability\n",
    "# Since we have sparse data (more on report)\n",
    "\n",
    "# Laplace smoothing value\n",
    "alpha = 1\n",
    "\n",
    "# Laplace function returns the conditional probability of a word given a class\n",
    "# with laplace smoothing\n",
    "def laplace(count, alpha, total, v):\n",
    "    return (count+alpha) / (total + v*alpha)\n",
    "\n",
    "# Calculates the likelihoods p_(c,i) of word i appearing in a given class c\n",
    "# returns the probability lists for each word in the vocab list and each class\n",
    "def calculate_likelihoods(count_c0, count_c1, vocab_dict, alpha=1):\n",
    "    # Initialise empty lists to store the likelihoods\n",
    "    class_0_c = []\n",
    "    class_1_c = []\n",
    "    # Calculate total count of words in each class\n",
    "    total_c0 = count_c0.sum()\n",
    "    total_c1 = count_c1.sum()\n",
    "    # Find the length of the vocab list\n",
    "    V = len(vocab_dict)\n",
    "    # For each class calculate the likelihood list\n",
    "    for word, index in vocab_dict.items():\n",
    "        count = count_c0[index]\n",
    "        # Find prob using laplace smoothing\n",
    "        p_ci = laplace(count, alpha, total_c0, V)\n",
    "        class_0_c.append(p_ci)\n",
    "    for word, index in vocab_dict.items():\n",
    "        count = count_c1[index]\n",
    "        # Find prob using laplace smoothing\n",
    "        p_ci = laplace(count, alpha, total_c1, V)\n",
    "        class_1_c.append(p_ci)\n",
    "    return class_0_c, class_1_c\n",
    "\n",
    "# Find the likelihoods for each class\n",
    "class_0_c, class_1_c = calculate_likelihoods(count_c0, count_c1, vocab_dict, alpha)\n",
    "\n",
    "\n",
    "# Check probabilities sum to roughly one\n",
    "print(sum(class_1_c))\n",
    "print(sum(class_0_c))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999736\n",
      "1.0000000000000056\n"
     ]
    }
   ],
   "execution_count": 302
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Further supervised model training questions for report"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.407109Z",
     "start_time": "2025-04-09T11:47:03.397372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Question 2\n",
    "# Find the most probable words in each class\n",
    "\n",
    "# We find the 10 most probable words by sorting the probabilities list\n",
    "# and finding their indexes in the original vocab list\n",
    "\n",
    "'''Finds and returns the n most probable words in a given class\n",
    "    returns a list in order of probability (descending) and a list\n",
    "     containing their probability values'''\n",
    "def find_n_most_probable(n, class_num):\n",
    "    # Find the count list for the given class\n",
    "    if class_num == 0:\n",
    "        prob_list = class_0_c\n",
    "    elif class_num == 1:\n",
    "        prob_list = class_1_c\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # Sort the list and get the indexes of the n most probable words\n",
    "    sorted_indexes = np.argsort(prob_list)[-n:][::-1]\n",
    "\n",
    "    sorted_probs = np.sort(prob_list)[-n:][::-1]\n",
    "\n",
    "    # Get the words from our vocab list using the indexes\n",
    "    most_probable_words = [vocab_list[i] for i in sorted_indexes]\n",
    "\n",
    "    return most_probable_words, sorted_probs\n",
    "\n",
    "\n",
    "# Find the 10 most probable words in class 0\n",
    "print(\"Most probable words in class 0:\", find_n_most_probable(10, 0)[0])\n",
    "\n",
    "# List their probability values\n",
    "print(\"Probability values for class 0:\", find_n_most_probable(10, 0)[1])\n",
    "\n",
    "# Find the 10 most probable words in class 1\n",
    "print(\"Most probable words in class 1:\", find_n_most_probable(10, 1)[0])\n",
    "print(\"Probability values for class 1:\", find_n_most_probable(10, 1)[1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable words in class 0: ['.', ',', '?', 'u', '...', '!', '..', ';', '&', 'go']\n",
      "Probability values for class 0: [0.07930378 0.02602418 0.02557645 0.0189165  0.0187486  0.01718155\n",
      " 0.01494291 0.013152   0.01309604 0.01113723]\n",
      "Most probable words in class 1: ['.', '!', ',', 'call', '£', 'free', '/', '2', '&', '?']\n",
      "Probability values for class 1: [0.05652174 0.02434783 0.02347826 0.02054348 0.01391304 0.01054348\n",
      " 0.00913043 0.00880435 0.00869565 0.00847826]\n"
     ]
    }
   ],
   "execution_count": 303
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Question 3: Finding the most predictive words\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.430346Z",
     "start_time": "2025-04-09T11:47:03.421074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the probability ratio of a word occurring in class 0 vs class 1\n",
    "class_0_c = np.array(class_0_c)\n",
    "class_1_c = np.array(class_1_c)\n",
    "p_ratio_non_mal = class_0_c/class_1_c\n",
    "\n",
    "# We don't have to worry about dividing by zero as we have already applied laplace smoothing\n",
    "\n",
    "# Find the 10 most strongly predictive words of the non-malicious class\n",
    "sorted_c0_div_c1 = np.argsort(p_ratio_non_mal)[-10:][::-1]\n",
    "most_predictive_words_non_mal = [vocab_list[i] for i in sorted_c0_div_c1]\n",
    "non_scam_ratios = np.sort(p_ratio_non_mal)[-10:][::-1]\n",
    "\n",
    "# Find the 10 most strongly predictive words of the malicious class\n",
    "p_ratio_mal = class_1_c/class_0_c\n",
    "sorted_c1_div_c0 = np.argsort(p_ratio_mal)[-10:][::-1]\n",
    "most_predictive_words_mal = [vocab_list[i] for i in sorted_c1_div_c0]\n",
    "scam_ratios = np.sort(p_ratio_mal)[-10:][::-1]\n",
    "\n",
    "# Print the most predictive words of the non-malicious class\n",
    "print(\"The most predictive words of the non-malicious class are:\", most_predictive_words_non_mal)\n",
    "print(\"The associated probability ratios are:\", non_scam_ratios)\n",
    "\n",
    "\n",
    "\n",
    "# Print the most predictive words of the malicious class\n",
    "print(\"The most predictive words of the malicious class are:\", most_predictive_words_mal)\n",
    "print(\"The associated probability ratios are:\", scam_ratios)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most predictive words of the non-malicious class are: [';', '...', 'gt', 'lt', ':)', 'ü', 'lor', 'hope', 'ok', 'd']\n",
      "The associated probability ratios are: [60.49921648 57.49570928 54.06312962 53.54824267 47.88448623 31.92299082\n",
      " 28.83366913 24.71457354 24.71457354 21.1103649 ]\n",
      "The most predictive words of the malicious class are: ['prize', 'tone', '£', 'select', 'claim', 'paytm', 'code', 'award', 'won', '18']\n",
      "The associated probability ratios are: [99.05086957 64.09173913 49.71965217 46.61217391 45.96478261 36.90130435\n",
      " 34.95913043 32.04586957 31.07478261 29.1326087 ]\n"
     ]
    }
   ],
   "execution_count": 304
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Supervised model evaluation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Predicting the labels of our test set"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.480637Z",
     "start_time": "2025-04-09T11:47:03.471793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read in our test dataset\n",
    "test_df = pd.read_csv('sms_test.csv')\n"
   ],
   "outputs": [],
   "execution_count": 305
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.539710Z",
     "start_time": "2025-04-09T11:47:03.530627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the same preprocessing steps as before\n",
    "# Ensure all values in the column are strings\n",
    "test_df['textPreprocessed'] = test_df['textPreprocessed'].astype(str)\n",
    "\n",
    "# Ensure no null values affect tokenising\n",
    "# Find num rows\n",
    "test_rows = test_df.shape[0]\n",
    "print(\"Number of entries before dropping null values: \", test_rows)\n",
    "test_df = test_df.dropna(subset=['textPreprocessed'])\n",
    "print(\"Number of entries before after dropping null values: \", test_rows)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before dropping null values:  1000\n",
      "Number of entries before after dropping null values:  1000\n"
     ]
    }
   ],
   "execution_count": 306
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.721502Z",
     "start_time": "2025-04-09T11:47:03.698808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For each test instance, compute a count vector\n",
    "# which represents the number of times each word in the vocab list appears\n",
    "# This will be in the form of an N_test x V matrix\n",
    "def build_test_count_matrix(test_df, vocab_list, vocab_dict):\n",
    "    # Initialise empty count matrix\n",
    "    test_count_matrix = np.zeros((test_df.shape[0], len(vocab_list)))\n",
    "    # Counters and sets for question 2\n",
    "    unique_words_in_vocab = set()\n",
    "    unique_words_not_in_vocab = set()\n",
    "    num_test_words_in_vocab = 0\n",
    "    num_test_words_not_in_vocab = 0\n",
    "\n",
    "    # Add words that exist in our vocab to the new count matrix for each row in the data\n",
    "    for index, text in enumerate(test_df['textPreprocessed']):\n",
    "        for word in text.split():\n",
    "            if word in vocab_dict:\n",
    "                word_index = vocab_dict.get(word)\n",
    "                test_count_matrix[index][word_index] += 1\n",
    "                unique_words_in_vocab.add(word)\n",
    "                num_test_words_in_vocab += 1\n",
    "            else:\n",
    "                unique_words_not_in_vocab.add(word)\n",
    "                num_test_words_not_in_vocab += 1\n",
    "\n",
    "    return test_count_matrix, num_test_words_in_vocab, num_test_words_not_in_vocab, unique_words_in_vocab, unique_words_not_in_vocab\n",
    "\n",
    "\n",
    "\n",
    "# Build the test count matrix\n",
    "test_count_matrix, num_test_words_in_vocab, num_test_words_not_in_vocab, unique_words_in_vocab, unique_words_not_in_vocab = build_test_count_matrix(test_df, vocab_list, vocab_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of unique words and in the vocab list and test (for question 2)\n",
    "num_unique_words_in_vocab = len(unique_words_in_vocab)\n",
    "num_unique_words_not_in_vocab = len(unique_words_not_in_vocab)\n",
    "num_test_words_total = num_test_words_in_vocab + num_test_words_not_in_vocab\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 307
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:03.738782Z",
     "start_time": "2025-04-09T11:47:03.726491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if all test instances contain words from the vocab list\n",
    "# If an instance contains no words from the vocab list, we won't classify it\n",
    "\n",
    "for row in test_count_matrix:\n",
    "    if row.sum() == 0:\n",
    "        print(\"Found a row with no words from the vocab list\")\n",
    "        print(row)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 308
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "No test items skipped as this code does not output any rows (refer to q2 below)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute the posterior probability of each class given the observed word count"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.258538Z",
     "start_time": "2025-04-09T11:47:03.784628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For each test instance, we compute the log posterior probability of observing each\n",
    "# word given the class label. Returns a list of log posterior probabilities for each class and probabilities\n",
    "def calc_posterior_log(count_matrix, likelihoods_list, label_index, priors):\n",
    "    # Initialise empty list to store posterior probabilities\n",
    "    posterior_probs = []\n",
    "    log_posterior_probs = []\n",
    "    prior_value = priors[label_index]\n",
    "\n",
    "    # For each test instance\n",
    "    for row in count_matrix:\n",
    "        # Initialise posterior probability\n",
    "        posterior = np.log(prior_value)\n",
    "\n",
    "        # For each word in the vocab list\n",
    "        for i, word_freq in enumerate(row):\n",
    "            if word_freq > 0:\n",
    "                # Get the likelihood of the word given the class label\n",
    "                likelihood_value = likelihoods_list[i]\n",
    "\n",
    "                # Take the log of this value to combat overflow\n",
    "                log_likelihood_value = np.log(likelihood_value)\n",
    "\n",
    "\n",
    "                # Update the posterior probability\n",
    "                posterior += (log_likelihood_value * word_freq)\n",
    "\n",
    "\n",
    "        # Find the log multinomial coefficient\n",
    "        # Using log gamma function to avoid overflow\n",
    "        log_multinomial_coefficient = (\n",
    "            math.lgamma(row.sum() + 1) - np.sum([math.lgamma(c + 1) for c in row])\n",
    "        )\n",
    "\n",
    "        posterior += log_multinomial_coefficient\n",
    "\n",
    "        # Add the posterior probability to our list\n",
    "        log_posterior_probs.append(posterior)\n",
    "        # Convert log posterior to normal posterior\n",
    "        posterior_probs.append(np.exp(posterior))\n",
    "\n",
    "    return log_posterior_probs, posterior_probs\n",
    "\n",
    "# Class 0 posterior\n",
    "log_c0_post, c0_posterior = calc_posterior_log(test_count_matrix, class_0_c, 0, priors)\n",
    "\n",
    "# Class 1 posterior\n",
    "\n",
    "log_c1_post, c1_posterior = calc_posterior_log(test_count_matrix, class_1_c, 1, priors)\n",
    "\n",
    "\n",
    "print(max(c1_posterior))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011304347826086959\n"
     ]
    }
   ],
   "execution_count": 309
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Predict the labels of each test instance"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.297062Z",
     "start_time": "2025-04-09T11:47:05.291885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Predicts the labels of each test instance using the log posterior probabilities '''\n",
    "def predict_nb(c0_posterior, c1_posterior):\n",
    "\n",
    "    # Using our previously calculated log posterior probabilities:\n",
    "\n",
    "    # Find argmax for each instance\n",
    "    argmax_labels = []\n",
    "    associated_probs = []\n",
    "\n",
    "    for post_prob_0, post_prob_1 in zip(c0_posterior, c1_posterior):\n",
    "        # Find the maximum posterior probability\n",
    "        if post_prob_0 >= post_prob_1:\n",
    "            associated_probs.append(post_prob_0)\n",
    "            argmax_labels.append(0)\n",
    "\n",
    "        # Otherwise if probabilities are less we classify as 1\n",
    "        else:\n",
    "            associated_probs.append(post_prob_1)\n",
    "            argmax_labels.append(1)\n",
    "\n",
    "    return argmax_labels, associated_probs\n",
    "\n",
    "\n",
    "# Run on our dataset\n",
    "pred_labels, probs = predict_nb(log_c0_post, log_c1_post)\n",
    "\n",
    "print(pred_labels[1:20])\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "execution_count": 310
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Find the accuracy of our model (Q1)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:00:54.596317Z",
     "start_time": "2025-04-09T14:00:54.590886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Overall accuracy of our classifier\n",
    "\n",
    "def get_accuracy(labels, true_labels):\n",
    "    # Find the number of correct predictions\n",
    "    num_correct = np.sum(labels == true_labels)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = num_correct / len(true_labels)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Get the true labels\n",
    "true_labels = test_df['class'].values\n",
    "\n",
    "# Prints the accuracy, confusion matrix, precision and recall\n",
    "# of our model's predictions\n",
    "def model_eval(true_labels, pred_labels):\n",
    "    accuracy = get_accuracy(true_labels, pred_labels)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion_matrix = np.zeros((2, 2), dtype=int)\n",
    "    # Fill the confusion matrix\n",
    "    for true, predicted in zip(true_labels, pred_labels):\n",
    "        confusion_matrix[true][predicted] += 1\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
    "\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "    TP = confusion_matrix[1, 1]\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Evaluate our model\n",
    "model_eval(true_labels, pred_labels)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "Confusion Matrix:\n",
      " [[785  15]\n",
      " [ 10 190]]\n",
      "10\n",
      "Precision: 0.93\n",
      "Recall: 0.95\n"
     ]
    }
   ],
   "execution_count": 333
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### How often we encountered out of vocab words (Q2)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.428062Z",
     "start_time": "2025-04-09T11:47:05.423389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_vocab_proportions(num_unique_words_not_in_vocab, num_unique_words_in_vocab, num_test_words_in_vocab, num_test_words_total):\n",
    "    # Proportion of unique words in the test set that were not in the vocab list\n",
    "    prop_unique_not_in_vocab = num_unique_words_not_in_vocab / (num_unique_words_not_in_vocab + num_unique_words_in_vocab)\n",
    "    prop_unique_in_vocab = 1 - prop_unique_not_in_vocab\n",
    "\n",
    "    print(f\"Proportion of unique words in vocab: {prop_unique_in_vocab}\")\n",
    "    print(f\"Proportion of unique words not in vocab: {prop_unique_not_in_vocab}\")\n",
    "\n",
    "    # Proportion of words in the test set that were not in the vocab list\n",
    "    prop_in_vocab = num_test_words_in_vocab / num_test_words_total\n",
    "    prop_not_in_vocab = 1 - prop_in_vocab\n",
    "\n",
    "    print(f\"Proportion of test words in vocab: {prop_in_vocab}\")\n",
    "    print(f\"Proportion of test words not in vocab: {prop_not_in_vocab}\")\n",
    "\n",
    "find_vocab_proportions(num_unique_words_not_in_vocab, num_unique_words_in_vocab, num_test_words_in_vocab, num_test_words_total)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of unique words in vocab: 0.9427178549664839\n",
      "Proportion of unique words not in vocab: 0.05728214503351615\n",
      "Proportion of test words in vocab: 0.9836797957695113\n",
      "Proportion of test words not in vocab: 0.016320204230488744\n"
     ]
    }
   ],
   "execution_count": 312
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confidence of Classification (Q3)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.452259Z",
     "start_time": "2025-04-09T11:47:05.447014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For all test instances, divide the posterior likelihoods for each class\n",
    "\n",
    "# Initialise empty lists to store the ratios\n",
    "likelihood_list_c0 = []\n",
    "likelihood_list_c1 =[]\n",
    "\n",
    "# For each test instance, we compute the ratio of the posterior probabilities\n",
    "for post_prob_0, post_prob_1 in zip(c0_posterior, c1_posterior):\n",
    "    likelihood_list_c0.append(post_prob_0 / post_prob_1)\n",
    "    likelihood_list_c1.append(post_prob_1 / post_prob_0)\n",
    "\n",
    "# Find the 3 most confident classifications for c0 (non-malicious)\n",
    "c0_indexes = np.argsort(likelihood_list_c0)[-3:][::-1]\n",
    "c0_ratios = np.sort(likelihood_list_c0)[-3:][::-1]\n",
    "instances_c0 = test_df['textPreprocessed'].iloc[c0_indexes]\n",
    "\n",
    "print(c0_ratios)\n",
    "print(instances_c0)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.13499445e+37 2.69038186e+29 3.18292454e+25]\n",
      "341    time : rs. transaction number & & & & & & & & ...\n",
      "223    ? ? ? ? .. .. u u u u , , ... ... ... ... say ...\n",
      "969    . every & & & & & & ; ; ; ; ; ; lt lt lt # # #...\n",
      "Name: textPreprocessed, dtype: object\n"
     ]
    }
   ],
   "execution_count": 313
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.491901Z",
     "start_time": "2025-04-09T11:47:05.486254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find the 3 most confident classifications for c1 (malicious)\n",
    "c1_indexes = np.argsort(likelihood_list_c1)[-3:][::-1]\n",
    "c1_ratios = np.sort(likelihood_list_c1)[-3:][::-1]\n",
    "instances_c1 = test_df['textPreprocessed'].iloc[c1_indexes]\n",
    "\n",
    "print(c1_ratios)\n",
    "print(instances_c1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.35388960e+20 1.28709054e+20 1.14912397e+20]\n",
      "844    . 4 + call £ - * holiday & urgent 18 t landlin...\n",
      "985    . 3 4 + ! call : £ offer * holiday & urgent 18...\n",
      "460    . . . , please order text call / : customer to...\n",
      "Name: textPreprocessed, dtype: object\n"
     ]
    }
   ],
   "execution_count": 314
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.531075Z",
     "start_time": "2025-04-09T11:47:05.525966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On the boundary between the two classes\n",
    "# Find the 3 text instances with R values closest to 1\n",
    "close_to_1_indexes = sorted(range(len(likelihood_list_c0)), key=lambda i: abs(likelihood_list_c0[i] - 1))[:3]\n",
    "close_to_1_ratios = [likelihood_list_c0[i] for i in close_to_1_indexes]\n",
    "\n",
    "instances_close_to_1 = test_df['textPreprocessed'].iloc[close_to_1_indexes]\n",
    "\n",
    "print(close_to_1_ratios)\n",
    "print(instances_close_to_1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0170981352199624, 1.0441124738285466, 0.9297658433847452]\n",
      "90                  . call dear\n",
      "455                . reply glad\n",
      "767    . . tell return re order\n",
      "Name: textPreprocessed, dtype: object\n"
     ]
    }
   ],
   "execution_count": 315
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:15:10.447417Z",
     "start_time": "2025-04-09T13:15:10.441377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "data = {\n",
    "    \"R Values Closest to 1\": close_to_1_ratios,\n",
    "    \"Text Instances\": instances_close_to_1.values\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Output the DataFrame to a CSV file\n",
    "df.to_csv('boundary_instances.csv', index=False)\n",
    "\n",
    "print(\"Results have been written to 'boundary_instances.csv'\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results have been written to 'boundary_instances.csv'\n"
     ]
    }
   ],
   "execution_count": 330
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending the model with semi-supervised training"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Active Learning"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split the labelled data into a training and validation set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.571839Z",
     "start_time": "2025-04-09T11:47:05.566177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform stratified split\n",
    "sms_df_train, sms_df_validation = train_test_split(sms_df, test_size=0.2, random_state=1462474, stratify=sms_df['class'])\n"
   ],
   "outputs": [],
   "execution_count": 316
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We must read in the unlabelled data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.639097Z",
     "start_time": "2025-04-09T11:47:05.629321Z"
    }
   },
   "cell_type": "code",
   "source": "sms_unlabelled = pd.read_csv('sms_unlabelled.csv')",
   "outputs": [],
   "execution_count": 317
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.646934Z",
     "start_time": "2025-04-09T11:47:05.642091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First we select 200 instances by random\n",
    "sms_ul_sample = sms_unlabelled.sample(200, random_state=1462474)\n",
    "\n",
    "# Append the sampled instances to the training set\n",
    "sms_df_train = pd.concat([sms_df_train, sms_ul_sample], ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.687212Z",
     "start_time": "2025-04-09T11:47:05.679582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the data\n",
    "sms_df_train = preprocess_data(sms_df_train)\n",
    "sms_df_validation = preprocess_data(sms_df_validation)\n",
    "\n",
    "# For selecting predictive words later\n",
    "sms_unlabelled = preprocess_data(sms_unlabelled)\n"
   ],
   "outputs": [],
   "execution_count": 319
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Retrain our model on the new training set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:05.756645Z",
     "start_time": "2025-04-09T11:47:05.719607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrains our model by calculating the likelihoods and priors of the new training set\n",
    "def retrain_model(sms_df_train, alpha_new):\n",
    "    # Retrain our model\n",
    "    new_vocab_list = create_vocab_list(sms_df_train)\n",
    "\n",
    "    # Build count matrix\n",
    "    new_count_matrix, new_vocab_dict = build_count_matrix(sms_df_train, new_vocab_list)\n",
    "\n",
    "    # Calculate priors\n",
    "    priors = calculate_priors(sms_df_train)\n",
    "    p_c0_new = priors[0]\n",
    "    p_c1_new = priors[1]\n",
    "    print(priors)\n",
    "\n",
    "    # We need to find the number of times each word appears in each class\n",
    "    count_c0_new = count_words_in_class(0,sms_df_train, new_count_matrix)\n",
    "    count_c1_new = count_words_in_class(1,sms_df_train, new_count_matrix)\n",
    "    print(sum(count_c0_new))\n",
    "    print(sum(count_c0))\n",
    "\n",
    "    # Calculate likelihoods using laplace smoothing\n",
    "    class_0_c_new, class_1_c_new = calculate_likelihoods(count_c0_new, count_c1_new, new_vocab_dict, alpha_new)\n",
    "\n",
    "    return class_0_c_new, class_1_c_new, new_vocab_list, new_vocab_dict\n",
    "\n",
    "# Retrain our model\n",
    "alpha_new = 1\n",
    "class_0_c_new, class_1_c_new, new_vocab_list, new_vocab_dict = retrain_model(sms_df_train, alpha_new)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8027777777777778, 0.19722222222222222]\n",
      "14386.0\n",
      "15861.0\n"
     ]
    }
   ],
   "execution_count": 320
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model is trained now we must evaluate it on our validation set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:06.346979Z",
     "start_time": "2025-04-09T11:47:05.798305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluates our model on the validation set by predicting the labels of each test instance\n",
    "# and calculating the accuracy, confusion matrix, precision and recall\n",
    "def validation_eval(true_labels, class_0_c_new, class_1_c_new, validation_data, new_vocab_list, new_vocab_dict):\n",
    "\n",
    "    # Build the count matrix for the validation set\n",
    "    new_test_count_matrix, num_test_words_in_vocab, num_test_words_not_in_vocab, unique_words_in_vocab, unique_words_not_in_vocab = build_test_count_matrix(validation_data, new_vocab_list, new_vocab_dict)\n",
    "\n",
    "    # Count the number of unique words and in the vocab list and test (for question 2)\n",
    "    num_unique_words_in_vocab = len(unique_words_in_vocab)\n",
    "    num_unique_words_not_in_vocab = len(unique_words_not_in_vocab)\n",
    "    num_test_words_total = num_test_words_in_vocab + num_test_words_not_in_vocab\n",
    "\n",
    "    print(num_unique_words_not_in_vocab, num_unique_words_in_vocab, num_test_words_in_vocab, num_test_words_total)\n",
    "\n",
    "    # Class 0 posterior probabilities\n",
    "    log_c0_post, c0_posterior = calc_posterior_log(new_test_count_matrix, class_0_c_new, 0, priors)\n",
    "\n",
    "    # Class 1 posterior probabilities\n",
    "    log_c1_post, c1_posterior = calc_posterior_log(new_test_count_matrix, class_1_c_new, 1, priors)\n",
    "\n",
    "    # Predict the labels\n",
    "    pred_labels, probs = predict_nb(log_c0_post, log_c1_post)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model_eval(true_labels, pred_labels)\n",
    "\n",
    "\n",
    "# Call the function and evaluate the model\n",
    "validation_eval(sms_df_validation['class'].values, class_0_c_new, class_1_c_new, sms_df_validation, new_vocab_list, new_vocab_dict)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 1065 4479 4577\n",
      "Accuracy: 0.975\n",
      "Confusion Matrix:\n",
      " [[315   5]\n",
      " [  5  75]]\n",
      "Precision: 0.94\n",
      "Recall: 0.94\n"
     ]
    }
   ],
   "execution_count": 321
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Improving our model: Part 1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:09.162287Z",
     "start_time": "2025-04-09T11:47:06.410311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First we must look at the 200 instances added at random to our training set\n",
    "\n",
    "# Using the R value method we used earlier we will find the most predictive words\n",
    "# from our unlabelled data and select the most predictive instances\n",
    "\n",
    "# We will calculate the R value for each instance in the unlabelled data using our original trained model to predict posterior probabilities\n",
    "def unlabelled_predictiveness(sms_unlabelled, vocab_list, vocab_dict, likelihood_lists, priors, n):\n",
    "\n",
    "    # Create a count matrix for the unlabelled data\n",
    "    unlabelled_count_matrix, num_ul_words_in_vocab, num_ul_words_not_in_vocab, unique_words_in_vocab, unique_words_not_in_vocab = build_test_count_matrix(sms_unlabelled, vocab_list, vocab_dict)\n",
    "\n",
    "\n",
    "    # Find the posterior probabilities for each class\n",
    "    log_ul_posteriors_0, unlabelled_class_0_posteriors = calc_posterior_log(unlabelled_count_matrix, likelihood_lists[0], 0, priors)\n",
    "    log_ul_posteriors_1, unlabelled_class_1_posteriors = calc_posterior_log(unlabelled_count_matrix, likelihood_lists[1], 1, priors)\n",
    "\n",
    "\n",
    "    # Calculate the R value for each instance\n",
    "\n",
    "    # Initialise empty lists to store the R values\n",
    "    ul_r_0 = []\n",
    "    ul_r_1 = []\n",
    "\n",
    "    for i in range(len(unlabelled_count_matrix)):\n",
    "        # Calculate the R values using log operations\n",
    "        log_r_0 = log_ul_posteriors_0[i] - log_ul_posteriors_1[i]\n",
    "        log_r_1 = log_ul_posteriors_1[i] - log_ul_posteriors_0[i]\n",
    "\n",
    "        # Append the R values\n",
    "        ul_r_0.append(log_r_0)\n",
    "        ul_r_1.append(log_r_1)\n",
    "\n",
    "    \"\"\"\" Using our priors we split n (200) into the two classes, keeping the ratio of our classes as close as possible to the test data\n",
    "    # For class 0 we should include\n",
    "    n_c0 = int(n * priors[0])\n",
    "    # Ensure the total adds up to n (200)\n",
    "    n_c1 = n - n_c0\n",
    "\n",
    "    # Find the indexes of the n_c0 most predictive instances for class 0\n",
    "\n",
    "    c0_ul_indexes = np.argsort(ul_r_0)[-n_c0:][::-1]\n",
    "    c0_ul_ratios = np.sort(ul_r_0)[-n_c0:][::-1]\n",
    "\n",
    "    # Return the n_c0 most predictive instances\n",
    "    instances_ul_c0 = sms_unlabelled.iloc[c0_ul_indexes]\n",
    "\n",
    "    # Find the indexes of the n_c1 most predictive instances for class 1\n",
    "    c1_ul_indexes = np.argsort(ul_r_1)[-n_c1:][::-1]\n",
    "    c1_ul_ratios = np.sort(ul_r_1)[-n_c1:][::-1]\n",
    "\n",
    "    # Return the n_c1 most predictive instances\n",
    "    instances_ul_c1 = sms_unlabelled.iloc[c1_ul_indexes] \"\"\"\n",
    "\n",
    "    # Choosing the highest R values didn't work so we will choose the R values closest to 1\n",
    "    close_to_1_indexes_ul = sorted(range(len(ul_r_0)), key=lambda i: abs(ul_r_0[i] - 1))[:n]\n",
    "    close_to_1_ratios_ul = [ul_r_0[i] for i in close_to_1_indexes_ul]\n",
    "    instances_close_to_1 = sms_unlabelled.iloc[close_to_1_indexes_ul]\n",
    "\n",
    "\n",
    "\n",
    "    # Try entropies instead\n",
    "    # Entropy list\n",
    "    entropies =[]\n",
    "\n",
    "    for log_c_0_post, log_c_1_post in zip(log_ul_posteriors_0, log_ul_posteriors_1):\n",
    "\n",
    "        # Convert to probabilities for entropy calculation\n",
    "        c_0_post = np.exp(log_c_0_post)\n",
    "        c_1_post = np.exp(log_c_1_post)\n",
    "\n",
    "        # Ensure probabilities are normalised\n",
    "        total_post = c_0_post + c_1_post\n",
    "        c_0_post= c_0_post / total_post\n",
    "        c_1_post = c_1_post / total_post\n",
    "\n",
    "        # Calculate entropy\n",
    "        entropy = - (c_0_post * np.log2(c_0_post) + c_1_post * np.log2(c_1_post))\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    # Select the n instances with the highest entropy (most uncertain)\n",
    "    highest_entropy_indexes = np.argsort(entropies)[-n:][::-1]\n",
    "    instances_high_entropy = sms_unlabelled.iloc[highest_entropy_indexes]\n",
    "\n",
    "    return instances_high_entropy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find the 200 most predictive instances\n",
    "n = 200\n",
    "# Use the original trained model and priors to predict the posterior probabilities and get the predictive instances\n",
    "instances_to_add = unlabelled_predictiveness(sms_unlabelled, vocab_list, vocab_dict, [class_0_c, class_1_c], priors, n)\n"
   ],
   "outputs": [],
   "execution_count": 322
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retrain model using these 200 words instead"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Try sk learn train_test_split"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:09.168476Z",
     "start_time": "2025-04-09T11:47:09.165281Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split",
   "outputs": [],
   "execution_count": 323
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:09.786477Z",
     "start_time": "2025-04-09T11:47:09.201394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add the new instances to the training set\n",
    "sms_df = pd.read_csv(\"sms_supervised_train.csv\")\n",
    "\n",
    "# Split the labelled data into a training and validation set with stratification\n",
    "sms_df_improved_train, sms_df_improved_validation = train_test_split(sms_df, test_size=0.2, random_state=1462474, stratify=sms_df['class'] )\n",
    "# Upsample uncertain records by duplicating them\n",
    "upsampled_instances = pd.concat([instances_to_add] * 2, ignore_index=True)\n",
    "\n",
    "# Append the upsampled uncertain records to the training set\n",
    "sms_df_improved_train = pd.concat([sms_df_improved_train, upsampled_instances], ignore_index=True)\n",
    "sms_df_improved_train = sms_df_improved_train.reset_index(drop=True)\n",
    "\n",
    "# Preprocess the data\n",
    "sms_df_improved_train = preprocess_data(sms_df_improved_train)\n",
    "sms_df_improved_validation = preprocess_data(sms_df_improved_validation)\n",
    "\n",
    "# Retrain the model with the updated training set\n",
    "alpha_improved = 1\n",
    "class_0_c_new_i, class_1_c_new_i, new_vocab_list_i, new_vocab_dict_i = retrain_model(sms_df_improved_train, alpha_improved)\n",
    "\n",
    "# Evaluate the model\n",
    "validation_eval(\n",
    "    sms_df_improved_validation['class'].values,\n",
    "    class_0_c_new_i,\n",
    "    class_1_c_new_i,\n",
    "    sms_df_improved_validation,\n",
    "    new_vocab_list_i,\n",
    "    new_vocab_dict_i\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.806, 0.194]\n",
      "14224.0\n",
      "15861.0\n",
      "71 1063 4481 4577\n",
      "Accuracy: 0.9825\n",
      "Confusion Matrix:\n",
      " [[318   2]\n",
      " [  5  75]]\n",
      "Precision: 0.97\n",
      "Recall: 0.94\n"
     ]
    }
   ],
   "execution_count": 324
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test model on test data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:11.235561Z",
     "start_time": "2025-04-09T11:47:09.820295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate our model on the test set\n",
    "validation_eval(test_df['class'].values, class_0_c_new_i,\n",
    "                class_1_c_new_i, test_df, new_vocab_list_i, new_vocab_dict_i)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 1508 10716 10968\n",
      "Accuracy: 0.978\n",
      "Confusion Matrix:\n",
      " [[787  13]\n",
      " [  9 191]]\n",
      "Precision: 0.94\n",
      "Recall: 0.95\n"
     ]
    }
   ],
   "execution_count": 325
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:11.275217Z",
     "start_time": "2025-04-09T11:47:11.272573Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:11.346106Z",
     "start_time": "2025-04-09T11:47:11.343853Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:11.388566Z",
     "start_time": "2025-04-09T11:47:11.386360Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:11.429055Z",
     "start_time": "2025-04-09T11:47:11.427066Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:11.470363Z",
     "start_time": "2025-04-09T11:47:11.468428Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:11.479632Z",
     "start_time": "2025-04-09T11:47:11.477347Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
