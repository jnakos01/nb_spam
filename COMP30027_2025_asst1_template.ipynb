{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2025 Semester 1\n",
    "\n",
    "## Assignment 1: Scam detection with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Student ID(s):**     `1462474`\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, GRAPHS, AND FIGURES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).** Results, figures, etc. which appear in this file but are NOT included in your report will not be marked.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised model training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T13:49:30.203492Z",
     "start_time": "2025-03-30T13:49:30.200487Z"
    }
   },
   "source": [
    "## Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Read in supervised training dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T13:49:30.488940Z",
     "start_time": "2025-03-30T13:49:30.477548Z"
    }
   },
   "cell_type": "code",
   "source": "sms_df = pd.read_csv('sms_supervised_train.csv')",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reformat text to help with tokenising"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T13:49:30.767574Z",
     "start_time": "2025-03-30T13:49:30.759215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure all values in the column are strings\n",
    "sms_df['textPreprocessed'] = sms_df['textPreprocessed'].astype(str)\n",
    "\n",
    "# Ensure no null values affect tokenising\n",
    "# Find num rows\n",
    "n_rows = sms_df.shape[0]\n",
    "print(\"Number of entries before dropping null values: \", n_rows)\n",
    "sms_df = sms_df.dropna(subset=['textPreprocessed'])\n",
    "print(\"Number of entries before after dropping null values: \", n_rows)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before dropping null values:  2000\n",
      "Number of entries before after dropping null values:  2000\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we have 2000 values before and after, it is safe to conclude we have no null entries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T13:49:31.038625Z",
     "start_time": "2025-03-30T13:49:31.035012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure class data is of the same data type\n",
    "sms_df['class'] = sms_df['class'].astype(int)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build vocabulary list"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T13:49:31.352661Z",
     "start_time": "2025-03-30T13:49:31.345608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define vocab list (set for build efficiency)\n",
    "vocab_set = set()\n",
    "\n",
    "# Add each word to the vocabulary set\n",
    "for text in sms_df['textPreprocessed']:\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    vocab_set.update(words)\n",
    "\n",
    "# Convert our set into a list as required\n",
    "vocab_list = list(vocab_set)\n",
    "# Free the set to save memory\n",
    "del vocab_set"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build count matrix"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T13:49:31.663570Z",
     "start_time": "2025-03-30T13:49:31.633517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialise empty count matrix\n",
    "count_matrix = np.zeros((sms_df.shape[0], len(vocab_list)))\n",
    "\n",
    "\n",
    "# Make a vocab dictionary for faster lookups\n",
    "vocab_dict = {word: i for i, word in enumerate(vocab_list)}\n",
    "\n",
    "\n",
    "# Fill count matrix by looking at every word in each row\n",
    "# and counting how many times it appears\n",
    "for index,text in sms_df['textPreprocessed'].items():\n",
    "    for word in text.split():\n",
    "        if word in vocab_dict:\n",
    "            word_index = vocab_dict.get(word)\n",
    "            count_matrix[index][word_index] += 1\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute the prior probability of each class:\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T13:50:23.831222Z",
     "start_time": "2025-03-30T13:50:23.825227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The dataset has two classes, so we have two priors\n",
    "\n",
    "# Class 0 (non-malicious)\n",
    "n_c0 = sms_df[sms_df['class'] == 0].shape[0]\n",
    "\n",
    "# Calculate our prior\n",
    "p_c0 = n_c0/n_rows\n",
    "\n",
    "\n",
    "# Class 1 (malicious)\n",
    "n_c1 = sms_df[sms_df['class'] == 1].shape[0]\n",
    "\n",
    "# Calculate our prior\n",
    "p_c1 = n_c1/n_rows\n",
    "\n",
    "\n",
    "print(f\"Our two priors are p_c0 = {p_c0}\\nand p_c1 = {p_c1} \")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our two priors are p_c0 = 0.8\n",
      "and p_c1 = 0.2 \n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Find the probability of each word appearing in a message from each class"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We will use laplace smoothing to ensure every event has a non-zero probability\n",
    "# Since we have sparse data (more on report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Supervised model evaluation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-30T13:49:32.266968Z",
     "start_time": "2025-03-30T13:49:32.263880Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending the model with semi-supervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
