{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2025 Semester 1\n",
    "\n",
    "## Assignment 1: Scam detection with naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Student ID(s):**     `1462474`\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, GRAPHS, AND FIGURES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).** Results, figures, etc. which appear in this file but are NOT included in your report will not be marked.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised model training\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:43.560355Z",
     "start_time": "2025-04-10T14:56:42.988443Z"
    }
   },
   "source": [
    "## Import necessary libraries\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Read in supervised training dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:43.584288Z",
     "start_time": "2025-04-10T14:56:43.568332Z"
    }
   },
   "cell_type": "code",
   "source": "sms_df = pd.read_csv('sms_supervised_train.csv')",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Reformat text to help with tokenising"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.129511Z",
     "start_time": "2025-04-10T14:56:44.123049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensures data types are as intended and no nulls\n",
    "def preprocess_data(df):\n",
    "    df['textPreprocessed'] = df['textPreprocessed'].astype(str)\n",
    "    df = df.dropna(subset=['textPreprocessed'])\n",
    "    df['class'] = df['class'].astype(int)\n",
    "    return df\n",
    "\n",
    "sms_df = preprocess_data(sms_df)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build vocabulary list"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.144632Z",
     "start_time": "2025-04-10T14:56:44.138911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define vocab list (set for build efficiency)\n",
    "vocab_set = set()\n",
    "\n",
    "# For each row in the dataset, split text into words and add them to the vocab set\n",
    "# Convert to a list at the end\n",
    "def create_vocab_list(df):\n",
    "    vocab_set = set()\n",
    "    for text in df['textPreprocessed']:\n",
    "        words = text.split()\n",
    "        vocab_set.update(words)\n",
    "    return list(vocab_set)\n",
    "\n",
    "# Create vocab list\n",
    "vocab_list = create_vocab_list(sms_df)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build count matrix"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.231203Z",
     "start_time": "2025-04-10T14:56:44.206044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creates a count matrix where each row represents a text instance\n",
    "# Columns represent the words in the vocab list.\n",
    "# Returns the matrix and a dictionary mapping words to their index in the vocab list\n",
    "def build_count_matrix(df, vocab_list):\n",
    "    # Initialise empty count matrix\n",
    "    count_matrix = np.zeros((df.shape[0], len(vocab_list)))\n",
    "    # Create dictionary mapping words to their index in the vocab list\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab_list)}\n",
    "    for index, text in df['textPreprocessed'].items():\n",
    "        for word in text.split():\n",
    "            # If the word is in the vocab list, increment its count in the matrix\n",
    "            if word in vocab_dict:\n",
    "                word_index = vocab_dict.get(word)\n",
    "                count_matrix[index][word_index] += 1\n",
    "    return count_matrix, vocab_dict\n",
    "\n",
    "# Build count matrix\n",
    "count_matrix,vocab_dict = build_count_matrix(sms_df, vocab_list)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute the prior probability of each class:\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.246084Z",
     "start_time": "2025-04-10T14:56:44.239178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the prior probability of each class given the dataframe\n",
    "def calculate_priors(df):\n",
    "    n_rows = df.shape[0]\n",
    "    # Count the number of instances in each class\n",
    "    n_c0 = df[df['class'] == 0].shape[0]\n",
    "    n_c1 = df[df['class'] == 1].shape[0]\n",
    "    # Calculate the prior probability of each class\n",
    "    p_c0 = n_c0 / n_rows\n",
    "    p_c1 = n_c1 / n_rows\n",
    "    return [p_c0, p_c1]\n",
    "\n",
    "# Calculate priors\n",
    "priors = calculate_priors(sms_df)\n",
    "p_c0 = priors[0]\n",
    "p_c1 = priors[1]\n",
    "\n",
    "# Answer to question 1 in our report\n",
    "print(f\"Our two priors are p_c0 = {p_c0}\\nand p_c1 = {p_c1} \")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our two priors are p_c0 = 0.8\n",
      "and p_c1 = 0.2 \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Find the probability of each word appearing in a message from each class (likelihood)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.319533Z",
     "start_time": "2025-04-10T14:56:44.301316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First we need to find the number of times each word appears in each class\n",
    "\n",
    "''' Counts the number of times each word appears in a given class\n",
    "    returns an list of counts '''\n",
    "def count_words_in_class(class_num, train_df, count_matrix):\n",
    "    # Find the indexes of the rows in the count matrix that belong to the given class\n",
    "    index_list = np.where(train_df['class'] == class_num)[0]\n",
    "    # Get the count matrix for the given class (only shows rows where we classify as class_num)\n",
    "    class_count_matrix = count_matrix[index_list]\n",
    "    # Sums the word counts across all text instances for the class\n",
    "    count_list = class_count_matrix.sum(axis=0)\n",
    "    return count_list\n",
    "\n",
    "# Class 0 count\n",
    "count_c0 = count_words_in_class(0, sms_df, count_matrix)\n",
    "\n",
    "# Class 1 count\n",
    "count_c1 = count_words_in_class(1, sms_df, count_matrix)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.336069Z",
     "start_time": "2025-04-10T14:56:44.325514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We will use laplace smoothing to ensure every event has a non-zero probability\n",
    "# Since we have sparse data (more on report)\n",
    "\n",
    "# Laplace smoothing value\n",
    "alpha = 1\n",
    "\n",
    "# Laplace function returns the conditional probability of a word given a class\n",
    "# with laplace smoothing\n",
    "def laplace(count, alpha, total, v):\n",
    "    return (count+alpha) / (total + v*alpha)\n",
    "\n",
    "# Calculates the likelihoods p_(c,i) of word i appearing in a given class c\n",
    "# returns the probability lists for each word in the vocab list and each class\n",
    "def calculate_likelihoods(count_c0, count_c1, vocab_dict, alpha=1):\n",
    "    # Initialise empty lists to store the likelihoods\n",
    "    class_0_c = []\n",
    "    class_1_c = []\n",
    "    # Calculate total count of words in each class\n",
    "    total_c0 = count_c0.sum()\n",
    "    total_c1 = count_c1.sum()\n",
    "    # Find the length of the vocab list\n",
    "    V = len(vocab_dict)\n",
    "    # For each class calculate the likelihood list\n",
    "    for word, index in vocab_dict.items():\n",
    "        count = count_c0[index]\n",
    "        # Find prob using laplace smoothing\n",
    "        p_ci = laplace(count, alpha, total_c0, V)\n",
    "        class_0_c.append(p_ci)\n",
    "    for word, index in vocab_dict.items():\n",
    "        count = count_c1[index]\n",
    "        # Find prob using laplace smoothing\n",
    "        p_ci = laplace(count, alpha, total_c1, V)\n",
    "        class_1_c.append(p_ci)\n",
    "    return class_0_c, class_1_c\n",
    "\n",
    "# Find the likelihoods for each class\n",
    "class_0_c, class_1_c = calculate_likelihoods(count_c0, count_c1, vocab_dict, alpha)\n",
    "\n",
    "\n",
    "# Check probabilities sum to roughly one\n",
    "print(sum(class_1_c))\n",
    "print(sum(class_0_c))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999999999999977\n",
      "1.0000000000000044\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Further supervised model training questions for report"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.382883Z",
     "start_time": "2025-04-10T14:56:44.374904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Question 2\n",
    "# Find the most probable words in each class\n",
    "\n",
    "# We find the 10 most probable words by sorting the probabilities list\n",
    "# and finding their indexes in the original vocab list\n",
    "\n",
    "'''Finds and returns the n most probable words in a given class\n",
    "    returns a list in order of probability (descending) and a list\n",
    "     containing their probability values'''\n",
    "def find_n_most_probable(n, class_num):\n",
    "    # Find the count list for the given class\n",
    "    if class_num == 0:\n",
    "        prob_list = class_0_c\n",
    "    elif class_num == 1:\n",
    "        prob_list = class_1_c\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # Sort the list and get the indexes of the n most probable words\n",
    "    sorted_indexes = np.argsort(prob_list)[-n:][::-1]\n",
    "\n",
    "    sorted_probs = np.sort(prob_list)[-n:][::-1]\n",
    "\n",
    "    # Get the words from our vocab list using the indexes\n",
    "    most_probable_words = [vocab_list[i] for i in sorted_indexes]\n",
    "\n",
    "    return most_probable_words, sorted_probs\n",
    "\n",
    "\n",
    "# Find the 10 most probable words in class 0\n",
    "print(\"Most probable words in class 0:\", find_n_most_probable(10, 0)[0])\n",
    "\n",
    "# List their probability values\n",
    "print(\"Probability values for class 0:\", find_n_most_probable(10, 0)[1])\n",
    "\n",
    "# Find the 10 most probable words in class 1\n",
    "print(\"Most probable words in class 1:\", find_n_most_probable(10, 1)[0])\n",
    "print(\"Probability values for class 1:\", find_n_most_probable(10, 1)[1])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable words in class 0: ['.', ',', '?', 'u', '...', '!', '..', ';', '&', 'go']\n",
      "Probability values for class 0: [0.07930378 0.02602418 0.02557645 0.0189165  0.0187486  0.01718155\n",
      " 0.01494291 0.013152   0.01309604 0.01113723]\n",
      "Most probable words in class 1: ['.', '!', ',', 'call', '£', 'free', '/', '2', '&', '?']\n",
      "Probability values for class 1: [0.05652174 0.02434783 0.02347826 0.02054348 0.01391304 0.01054348\n",
      " 0.00913043 0.00880435 0.00869565 0.00847826]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Question 3: Finding the most predictive words\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.427671Z",
     "start_time": "2025-04-10T14:56:44.419092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the probability ratio of a word occurring in class 0 vs class 1\n",
    "class_0_c = np.array(class_0_c)\n",
    "class_1_c = np.array(class_1_c)\n",
    "p_ratio_non_mal = class_0_c/class_1_c\n",
    "\n",
    "# We don't have to worry about dividing by zero as we have already applied laplace smoothing\n",
    "\n",
    "# Find the 10 most strongly predictive words of the non-malicious class\n",
    "sorted_c0_div_c1 = np.argsort(p_ratio_non_mal)[-10:][::-1]\n",
    "most_predictive_words_non_mal = [vocab_list[i] for i in sorted_c0_div_c1]\n",
    "non_scam_ratios = np.sort(p_ratio_non_mal)[-10:][::-1]\n",
    "\n",
    "# Find the 10 most strongly predictive words of the malicious class\n",
    "p_ratio_mal = class_1_c/class_0_c\n",
    "sorted_c1_div_c0 = np.argsort(p_ratio_mal)[-10:][::-1]\n",
    "most_predictive_words_mal = [vocab_list[i] for i in sorted_c1_div_c0]\n",
    "scam_ratios = np.sort(p_ratio_mal)[-10:][::-1]\n",
    "\n",
    "# Print the most predictive words of the non-malicious class\n",
    "print(\"The most predictive words of the non-malicious class are:\", most_predictive_words_non_mal)\n",
    "print(\"The associated probability ratios are:\", non_scam_ratios)\n",
    "\n",
    "\n",
    "\n",
    "# Print the most predictive words of the malicious class\n",
    "print(\"The most predictive words of the malicious class are:\", most_predictive_words_mal)\n",
    "print(\"The associated probability ratios are:\", scam_ratios)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most predictive words of the non-malicious class are: [';', '...', 'gt', 'lt', ':)', 'ü', 'lor', 'hope', 'ok', 'd']\n",
      "The associated probability ratios are: [60.49921648 57.49570928 54.06312962 53.54824267 47.88448623 31.92299082\n",
      " 28.83366913 24.71457354 24.71457354 21.1103649 ]\n",
      "The most predictive words of the malicious class are: ['prize', 'tone', '£', 'select', 'claim', 'paytm', 'code', 'award', 'won', '18']\n",
      "The associated probability ratios are: [99.05086957 64.09173913 49.71965217 46.61217391 45.96478261 36.90130435\n",
      " 34.95913043 32.04586957 31.07478261 29.1326087 ]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Supervised model evaluation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Predicting the labels of our test set"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.461158Z",
     "start_time": "2025-04-10T14:56:44.452319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read in our test dataset\n",
    "test_df = pd.read_csv('sms_test.csv')\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.493641Z",
     "start_time": "2025-04-10T14:56:44.486660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the same preprocessing steps as before\n",
    "# Ensure all values in the column are strings\n",
    "test_df['textPreprocessed'] = test_df['textPreprocessed'].astype(str)\n",
    "\n",
    "# Ensure no null values affect tokenising\n",
    "# Find num rows\n",
    "test_rows = test_df.shape[0]\n",
    "print(\"Number of entries before dropping null values: \", test_rows)\n",
    "test_df = test_df.dropna(subset=['textPreprocessed'])\n",
    "print(\"Number of entries before after dropping null values: \", test_rows)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries before dropping null values:  1000\n",
      "Number of entries before after dropping null values:  1000\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.548003Z",
     "start_time": "2025-04-10T14:56:44.531213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For each test instance, compute a count vector\n",
    "# which represents the number of times each word in the vocab list appears\n",
    "# This will be in the form of an N_test x V matrix\n",
    "def build_test_count_matrix(test_df, vocab_list, vocab_dict):\n",
    "    # Initialise empty count matrix\n",
    "    test_count_matrix = np.zeros((test_df.shape[0], len(vocab_list)))\n",
    "    # Counters and sets for question 2\n",
    "    unique_words_in_vocab = set()\n",
    "    unique_words_not_in_vocab = set()\n",
    "    num_test_words_in_vocab = 0\n",
    "    num_test_words_not_in_vocab = 0\n",
    "\n",
    "    # Add words that exist in our vocab to the new count matrix for each row in the data\n",
    "    for index, text in enumerate(test_df['textPreprocessed']):\n",
    "        for word in text.split():\n",
    "            if word in vocab_dict:\n",
    "                word_index = vocab_dict.get(word)\n",
    "                test_count_matrix[index][word_index] += 1\n",
    "                unique_words_in_vocab.add(word)\n",
    "                num_test_words_in_vocab += 1\n",
    "            else:\n",
    "                unique_words_not_in_vocab.add(word)\n",
    "                num_test_words_not_in_vocab += 1\n",
    "\n",
    "    return test_count_matrix, num_test_words_in_vocab, num_test_words_not_in_vocab, unique_words_in_vocab, unique_words_not_in_vocab\n",
    "\n",
    "\n",
    "\n",
    "# Build the test count matrix\n",
    "test_count_matrix, num_test_words_in_vocab, num_test_words_not_in_vocab, unique_words_in_vocab, unique_words_not_in_vocab = build_test_count_matrix(test_df, vocab_list, vocab_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Count the number of unique words and in the vocab list and test (for question 2)\n",
    "num_unique_words_in_vocab = len(unique_words_in_vocab)\n",
    "num_unique_words_not_in_vocab = len(unique_words_not_in_vocab)\n",
    "num_test_words_total = num_test_words_in_vocab + num_test_words_not_in_vocab\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:44.594738Z",
     "start_time": "2025-04-10T14:56:44.584765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if all test instances contain words from the vocab list\n",
    "# If an instance contains no words from the vocab list, we won't classify it\n",
    "\n",
    "for row in test_count_matrix:\n",
    "    if row.sum() == 0:\n",
    "        print(\"Found a row with no words from the vocab list\")\n",
    "        print(row)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "No test items skipped as this code does not output any rows (refer to q2 below)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Compute the posterior probability of each class given the observed word count"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:45.962036Z",
     "start_time": "2025-04-10T14:56:44.617803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For each test instance, we compute the log posterior probability of observing each\n",
    "# word given the class label. Returns a list of log posterior probabilities for each class and probabilities\n",
    "def calc_posterior_log(count_matrix, likelihoods_list, label_index, priors):\n",
    "    # Initialise empty list to store posterior probabilities\n",
    "    posterior_probs = []\n",
    "    log_posterior_probs = []\n",
    "    prior_value = priors[label_index]\n",
    "\n",
    "    # For each test instance\n",
    "    for row in count_matrix:\n",
    "        # Initialise posterior probability\n",
    "        posterior = np.log(prior_value)\n",
    "\n",
    "        # For each word in the vocab list\n",
    "        for i, word_freq in enumerate(row):\n",
    "            if word_freq > 0:\n",
    "                # Get the likelihood of the word given the class label\n",
    "                likelihood_value = likelihoods_list[i]\n",
    "\n",
    "                # Take the log of this value to combat overflow\n",
    "                log_likelihood_value = np.log(likelihood_value)\n",
    "\n",
    "\n",
    "                # Update the posterior probability\n",
    "                posterior += (log_likelihood_value * word_freq)\n",
    "\n",
    "\n",
    "        # Find the log multinomial coefficient\n",
    "        # Using log gamma function to avoid overflow\n",
    "        log_multinomial_coefficient = (\n",
    "            math.lgamma(row.sum() + 1) - np.sum([math.lgamma(c + 1) for c in row])\n",
    "        )\n",
    "\n",
    "        posterior += log_multinomial_coefficient\n",
    "\n",
    "        # Add the posterior probability to our list\n",
    "        log_posterior_probs.append(posterior)\n",
    "        # Convert log posterior to normal posterior\n",
    "        posterior_probs.append(np.exp(posterior))\n",
    "\n",
    "    return log_posterior_probs, posterior_probs\n",
    "\n",
    "# Class 0 posterior\n",
    "log_c0_post, c0_posterior = calc_posterior_log(test_count_matrix, class_0_c, 0, priors)\n",
    "\n",
    "# Class 1 posterior\n",
    "\n",
    "log_c1_post, c1_posterior = calc_posterior_log(test_count_matrix, class_1_c, 1, priors)\n",
    "\n",
    "\n",
    "print(max(c1_posterior))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011304347826086959\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Predict the labels of each test instance"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:45.991305Z",
     "start_time": "2025-04-10T14:56:45.985758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "''' Predicts the labels of each test instance using the log posterior probabilities '''\n",
    "def predict_nb(c0_posterior, c1_posterior):\n",
    "\n",
    "    # Using our previously calculated log posterior probabilities:\n",
    "\n",
    "    # Find argmax for each instance\n",
    "    argmax_labels = []\n",
    "    associated_probs = []\n",
    "\n",
    "    for post_prob_0, post_prob_1 in zip(c0_posterior, c1_posterior):\n",
    "        # Find the maximum posterior probability\n",
    "        if post_prob_0 >= post_prob_1:\n",
    "            associated_probs.append(post_prob_0)\n",
    "            argmax_labels.append(0)\n",
    "\n",
    "        # Otherwise if probabilities are less we classify as 1\n",
    "        else:\n",
    "            associated_probs.append(post_prob_1)\n",
    "            argmax_labels.append(1)\n",
    "\n",
    "    return argmax_labels, associated_probs\n",
    "\n",
    "\n",
    "# Run on our dataset\n",
    "pred_labels, probs = predict_nb(log_c0_post, log_c1_post)\n",
    "\n",
    "print(pred_labels[1:20])\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Find the accuracy of our model (Q1)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.021005Z",
     "start_time": "2025-04-10T14:56:46.014189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Overall accuracy of our classifier\n",
    "\n",
    "def get_accuracy(labels, true_labels):\n",
    "    # Find the number of correct predictions\n",
    "    num_correct = np.sum(labels == true_labels)\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = num_correct / len(true_labels)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Get the true labels\n",
    "true_labels = test_df['class'].values\n",
    "\n",
    "# Prints the accuracy, confusion matrix, precision and recall\n",
    "# of our model's predictions\n",
    "def model_eval(true_labels, pred_labels):\n",
    "    accuracy = get_accuracy(true_labels, pred_labels)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Confusion matrix\n",
    "    confusion_matrix = np.zeros((2, 2), dtype=int)\n",
    "    # Fill the confusion matrix\n",
    "    for true, predicted in zip(true_labels, pred_labels):\n",
    "        confusion_matrix[true][predicted] += 1\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
    "\n",
    "    TN = confusion_matrix[0, 0]\n",
    "    FP = confusion_matrix[0, 1]\n",
    "    FN = confusion_matrix[1, 0]\n",
    "    TP = confusion_matrix[1, 1]\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# Evaluate our model\n",
    "model_eval(true_labels, pred_labels)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.975\n",
      "Confusion Matrix:\n",
      " [[785  15]\n",
      " [ 10 190]]\n",
      "Precision: 0.93\n",
      "Recall: 0.95\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### How often we encountered out of vocab words (Q2)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.055614Z",
     "start_time": "2025-04-10T14:56:46.051697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_vocab_proportions(num_unique_words_not_in_vocab, num_unique_words_in_vocab, num_test_words_in_vocab, num_test_words_total):\n",
    "    # Proportion of unique words in the test set that were not in the vocab list\n",
    "    prop_unique_not_in_vocab = num_unique_words_not_in_vocab / (num_unique_words_not_in_vocab + num_unique_words_in_vocab)\n",
    "    prop_unique_in_vocab = 1 - prop_unique_not_in_vocab\n",
    "\n",
    "    print(f\"Proportion of unique words in vocab: {prop_unique_in_vocab}\")\n",
    "    print(f\"Proportion of unique words not in vocab: {prop_unique_not_in_vocab}\")\n",
    "\n",
    "    # Proportion of words in the test set that were not in the vocab list\n",
    "    prop_in_vocab = num_test_words_in_vocab / num_test_words_total\n",
    "    prop_not_in_vocab = 1 - prop_in_vocab\n",
    "\n",
    "    print(f\"Proportion of test words in vocab: {prop_in_vocab}\")\n",
    "    print(f\"Proportion of test words not in vocab: {prop_not_in_vocab}\")\n",
    "\n",
    "find_vocab_proportions(num_unique_words_not_in_vocab, num_unique_words_in_vocab, num_test_words_in_vocab, num_test_words_total)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of unique words in vocab: 0.9427178549664839\n",
      "Proportion of unique words not in vocab: 0.05728214503351615\n",
      "Proportion of test words in vocab: 0.9836797957695113\n",
      "Proportion of test words not in vocab: 0.016320204230488744\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Confidence of Classification (Q3)"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.094955Z",
     "start_time": "2025-04-10T14:56:46.086951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For all test instances, divide the posterior likelihoods for each class\n",
    "\n",
    "# Initialise empty lists to store the ratios\n",
    "likelihood_list_c0 = []\n",
    "likelihood_list_c1 =[]\n",
    "\n",
    "# For each test instance, we compute the ratio of the posterior probabilities\n",
    "for post_prob_0, post_prob_1 in zip(c0_posterior, c1_posterior):\n",
    "    likelihood_list_c0.append(post_prob_0 / post_prob_1)\n",
    "    likelihood_list_c1.append(post_prob_1 / post_prob_0)\n",
    "\n",
    "# Find the 3 most confident classifications for c0 (non-malicious)\n",
    "c0_indexes = np.argsort(likelihood_list_c0)[-3:][::-1]\n",
    "c0_ratios = np.sort(likelihood_list_c0)[-3:][::-1]\n",
    "instances_c0 = test_df['textPreprocessed'].iloc[c0_indexes]\n",
    "\n",
    "print(c0_ratios)\n",
    "print(instances_c0)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.13499445e+37 2.69038186e+29 3.18292454e+25]\n",
      "341    time : rs. transaction number & & & & & & & & ...\n",
      "223    ? ? ? ? .. .. u u u u , , ... ... ... ... say ...\n",
      "969    . every & & & & & & ; ; ; ; ; ; lt lt lt # # #...\n",
      "Name: textPreprocessed, dtype: object\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.149430Z",
     "start_time": "2025-04-10T14:56:46.143804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Find the 3 most confident classifications for c1 (malicious)\n",
    "c1_indexes = np.argsort(likelihood_list_c1)[-3:][::-1]\n",
    "c1_ratios = np.sort(likelihood_list_c1)[-3:][::-1]\n",
    "instances_c1 = test_df['textPreprocessed'].iloc[c1_indexes]\n",
    "\n",
    "print(c1_ratios)\n",
    "print(instances_c1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.35388960e+20 1.28709054e+20 1.14912397e+20]\n",
      "844    . 4 + call £ - * holiday & urgent 18 t landlin...\n",
      "985    . 3 4 + ! call : £ offer * holiday & urgent 18...\n",
      "460    . . . , please order text call / : customer to...\n",
      "Name: textPreprocessed, dtype: object\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.190177Z",
     "start_time": "2025-04-10T14:56:46.184486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On the boundary between the two classes\n",
    "# Find the 3 text instances with R values closest to 1\n",
    "close_to_1_indexes = sorted(range(len(likelihood_list_c0)), key=lambda i: abs(likelihood_list_c0[i] - 1))[:3]\n",
    "close_to_1_ratios = [likelihood_list_c0[i] for i in close_to_1_indexes]\n",
    "\n",
    "instances_close_to_1 = test_df['textPreprocessed'].iloc[close_to_1_indexes]\n",
    "\n",
    "print(close_to_1_ratios)\n",
    "print(instances_close_to_1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.017098135219966, 1.0441124738285466, 0.9297658433847519]\n",
      "90                  . call dear\n",
      "455                . reply glad\n",
      "767    . . tell return re order\n",
      "Name: textPreprocessed, dtype: object\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extending the model with semi-supervised training"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Active Learning"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split the labelled data into a training and validation set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.254711Z",
     "start_time": "2025-04-10T14:56:46.243030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Original random split\n",
    "# Random sample\n",
    "sms_df_train = sms_df.sample(frac=0.8, random_state=1462474).reset_index(drop=True)\n",
    "\n",
    "# Create the validation set\n",
    "sms_df_validation = sms_df.drop(sms_df_train.index)\n",
    "\n",
    "\n",
    "# Used when testing random samples with stratified split\n",
    "\"\"\"\n",
    "# Perform stratified split\n",
    "sms_df_train, sms_df_validation = train_test_split(sms_df, test_size=0.2, random_state=1462474, stratify=sms_df['class'])\"\"\"\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Perform stratified split\\nsms_df_train, sms_df_validation = train_test_split(sms_df, test_size=0.2, random_state=1462474, stratify=sms_df['class'])\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We must read in the unlabelled data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.333621Z",
     "start_time": "2025-04-10T14:56:46.320656Z"
    }
   },
   "cell_type": "code",
   "source": "sms_unlabelled = pd.read_csv('sms_unlabelled.csv')",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.409648Z",
     "start_time": "2025-04-10T14:56:46.402016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First we select 200 instances by random\n",
    "sms_ul_sample = sms_unlabelled.sample(200, random_state=1462474)\n",
    "\n",
    "# Append the sampled instances to the training set\n",
    "sms_df_train = pd.concat([sms_df_train, sms_ul_sample], ignore_index=True)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.429774Z",
     "start_time": "2025-04-10T14:56:46.420615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the data\n",
    "sms_df_train = preprocess_data(sms_df_train)\n",
    "sms_df_validation = preprocess_data(sms_df_validation)\n",
    "\n",
    "# For selecting predictive words later\n",
    "sms_unlabelled = preprocess_data(sms_unlabelled)\n"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Retrain our model on the new training set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:46.503046Z",
     "start_time": "2025-04-10T14:56:46.460160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrains our model by calculating the likelihoods and priors of the new training set\n",
    "def retrain_model(sms_df_train, alpha_new):\n",
    "    # Retrain our model\n",
    "    new_vocab_list = create_vocab_list(sms_df_train)\n",
    "\n",
    "    # Build count matrix\n",
    "    new_count_matrix, new_vocab_dict = build_count_matrix(sms_df_train, new_vocab_list)\n",
    "\n",
    "    # Calculate priors\n",
    "    priors = calculate_priors(sms_df_train)\n",
    "    p_c0_new = priors[0]\n",
    "    p_c1_new = priors[1]\n",
    "    print(priors)\n",
    "\n",
    "    # We need to find the number of times each word appears in each class\n",
    "    count_c0_new = count_words_in_class(0,sms_df_train, new_count_matrix)\n",
    "    count_c1_new = count_words_in_class(1,sms_df_train, new_count_matrix)\n",
    "    print(sum(count_c0_new))\n",
    "    print(sum(count_c0))\n",
    "\n",
    "    # Calculate likelihoods using laplace smoothing\n",
    "    class_0_c_new, class_1_c_new = calculate_likelihoods(count_c0_new, count_c1_new, new_vocab_dict, alpha_new)\n",
    "\n",
    "    return class_0_c_new, class_1_c_new, new_vocab_list, new_vocab_dict\n",
    "\n",
    "# Retrain our model\n",
    "alpha_new = 1\n",
    "class_0_c_new, class_1_c_new, new_vocab_list, new_vocab_dict = retrain_model(sms_df_train, alpha_new)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7966666666666666, 0.20333333333333334]\n",
      "14444.0\n",
      "15861.0\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model is trained now we must evaluate it on our validation set"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:47.177319Z",
     "start_time": "2025-04-10T14:56:46.542251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluates our model on the validation set by predicting the labels of each test instance\n",
    "# and calculating the accuracy, confusion matrix, precision and recall\n",
    "def validation_eval(true_labels, class_0_c_new, class_1_c_new, validation_data, new_vocab_list, new_vocab_dict):\n",
    "\n",
    "    # Build the count matrix for the validation set\n",
    "    new_test_count_matrix, num_test_words_in_vocab, num_test_words_not_in_vocab, unique_words_in_vocab, unique_words_not_in_vocab = build_test_count_matrix(validation_data, new_vocab_list, new_vocab_dict)\n",
    "\n",
    "    # Count the number of unique words and in the vocab list and test (for question 2)\n",
    "    num_unique_words_in_vocab = len(unique_words_in_vocab)\n",
    "    num_unique_words_not_in_vocab = len(unique_words_not_in_vocab)\n",
    "    num_test_words_total = num_test_words_in_vocab + num_test_words_not_in_vocab\n",
    "\n",
    "    print(num_unique_words_not_in_vocab, num_unique_words_in_vocab, num_test_words_in_vocab, num_test_words_total)\n",
    "\n",
    "    # Class 0 posterior probabilities\n",
    "    log_c0_post, c0_posterior = calc_posterior_log(new_test_count_matrix, class_0_c_new, 0, priors)\n",
    "\n",
    "    # Class 1 posterior probabilities\n",
    "    log_c1_post, c1_posterior = calc_posterior_log(new_test_count_matrix, class_1_c_new, 1, priors)\n",
    "\n",
    "    # Predict the labels\n",
    "    pred_labels, probs = predict_nb(log_c0_post, log_c1_post)\n",
    "\n",
    "    # Evaluate the model\n",
    "    model_eval(true_labels, pred_labels)\n",
    "\n",
    "    return c0_posterior, c1_posterior\n",
    "\n",
    "\n",
    "# Call the function and evaluate the model\n",
    "_,_ = validation_eval(sms_df_validation['class'].values, class_0_c_new, class_1_c_new, sms_df_validation, new_vocab_list, new_vocab_dict)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1115 4586 4602\n",
      "Accuracy: 0.965\n",
      "Confusion Matrix:\n",
      " [[309   9]\n",
      " [  5  77]]\n",
      "Precision: 0.90\n",
      "Recall: 0.94\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Improving our model: Part 1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:50.325378Z",
     "start_time": "2025-04-10T14:56:47.207388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First we must look at the 200 instances added at random to our training set\n",
    "\n",
    "# Using the R value method we used earlier we will find the most predictive words\n",
    "# from our unlabelled data and select the most predictive instances\n",
    "\n",
    "# We will calculate the R value for each instance in the unlabelled data using our original trained model to predict posterior probabilities\n",
    "def unlabelled_predictiveness(sms_unlabelled, vocab_list, vocab_dict, likelihood_lists, priors, n):\n",
    "\n",
    "    # Create a count matrix for the unlabelled data\n",
    "    unlabelled_count_matrix, num_ul_words_in_vocab, num_ul_words_not_in_vocab, unique_words_in_vocab, unique_words_not_in_vocab = build_test_count_matrix(sms_unlabelled, vocab_list, vocab_dict)\n",
    "\n",
    "\n",
    "    # Find the posterior probabilities for each class\n",
    "    log_ul_posteriors_0, unlabelled_class_0_posteriors = calc_posterior_log(unlabelled_count_matrix, likelihood_lists[0], 0, priors)\n",
    "    log_ul_posteriors_1, unlabelled_class_1_posteriors = calc_posterior_log(unlabelled_count_matrix, likelihood_lists[1], 1, priors)\n",
    "\n",
    "\n",
    "    # Calculate the R value for each instance\n",
    "\n",
    "    # Initialise empty lists to store the R values\n",
    "    ul_r_0 = []\n",
    "    ul_r_1 = []\n",
    "\n",
    "    for i in range(len(unlabelled_count_matrix)):\n",
    "        # Calculate the R values using log operations\n",
    "        log_r_0 = log_ul_posteriors_0[i] - log_ul_posteriors_1[i]\n",
    "        log_r_1 = log_ul_posteriors_1[i] - log_ul_posteriors_0[i]\n",
    "\n",
    "        # Append the R values\n",
    "        ul_r_0.append(log_r_0)\n",
    "        ul_r_1.append(log_r_1)\n",
    "\n",
    "    # Choosing the highest R values didn't work so we will choose the R values closest to 1\n",
    "    close_to_1_indexes_ul = sorted(range(len(ul_r_0)), key=lambda i: abs(ul_r_0[i] - 1))[:n]\n",
    "    close_to_1_ratios_ul = [ul_r_0[i] for i in close_to_1_indexes_ul]\n",
    "    instances_close_to_1 = sms_unlabelled.iloc[close_to_1_indexes_ul]\n",
    "\n",
    "    # Try entropies instead\n",
    "    # Entropy list\n",
    "    entropies =[]\n",
    "\n",
    "    for log_c_0_post, log_c_1_post in zip(log_ul_posteriors_0, log_ul_posteriors_1):\n",
    "\n",
    "        # Convert to probabilities for entropy calculation\n",
    "        c_0_post = np.exp(log_c_0_post)\n",
    "        c_1_post = np.exp(log_c_1_post)\n",
    "\n",
    "        # Ensure probabilities are normalised\n",
    "        total_post = c_0_post + c_1_post\n",
    "        c_0_post= c_0_post / total_post\n",
    "        c_1_post = c_1_post / total_post\n",
    "\n",
    "        # Calculate entropy\n",
    "        entropy = - (c_0_post * np.log2(c_0_post) + c_1_post * np.log2(c_1_post))\n",
    "        entropies.append(entropy)\n",
    "\n",
    "    # Select the n instances with the highest entropy (most uncertain)\n",
    "    highest_entropy_indexes = np.argsort(entropies)[-n:][::-1]\n",
    "    instances_high_entropy = sms_unlabelled.iloc[highest_entropy_indexes]\n",
    "\n",
    "    # We are using high entropy as it performs slightly better than the R value method\n",
    "    return instances_high_entropy\n",
    "\n",
    "\n",
    "\n",
    "# Find the 200 most predictive instances\n",
    "n = 200\n",
    "# Use the original trained model and priors to predict the posterior probabilities and get the predictive instances\n",
    "instances_to_add = unlabelled_predictiveness(sms_unlabelled, vocab_list, vocab_dict, [class_0_c, class_1_c], priors, n)\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retrain model using these 200 words instead"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Try sk learn train_test_split"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:51.142342Z",
     "start_time": "2025-04-10T14:56:50.355296Z"
    }
   },
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:51.755166Z",
     "start_time": "2025-04-10T14:56:51.170689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add the new instances to the training set\n",
    "sms_df = pd.read_csv(\"sms_supervised_train.csv\")\n",
    "\n",
    "# Split the labelled data into a training and validation set with stratification\n",
    "sms_df_improved_train, sms_df_improved_validation = train_test_split(sms_df, test_size=0.2, random_state=1462474, stratify=sms_df['class'] )\n",
    "\n",
    "# Upsample uncertain records by duplicating them (not utilised in final results (as seen by * 1, just experimenting)\n",
    "upsampled_instances = pd.concat([instances_to_add] * 1, ignore_index=True)\n",
    "\n",
    "# Append the upsampled uncertain records to the training set\n",
    "sms_df_improved_train = pd.concat([sms_df_improved_train, upsampled_instances], ignore_index=True)\n",
    "sms_df_improved_train = sms_df_improved_train.reset_index(drop=True)\n",
    "\n",
    "# Preprocess the data\n",
    "sms_df_improved_train = preprocess_data(sms_df_improved_train)\n",
    "sms_df_improved_validation = preprocess_data(sms_df_improved_validation)\n",
    "\n",
    "# Retrain the model with the updated training set\n",
    "\n",
    "alpha_improved = 1\n",
    "class_0_c_new_i, class_1_c_new_i, new_vocab_list_i, new_vocab_dict_i = retrain_model(sms_df_improved_train, alpha_improved)\n",
    "\n",
    "# Evaluate the model\n",
    "c0_posteriors_i, c1_posteriors_i = validation_eval(\n",
    "    sms_df_improved_validation['class'].values,\n",
    "    class_0_c_new_i,\n",
    "    class_1_c_new_i,\n",
    "    sms_df_improved_validation,\n",
    "    new_vocab_list_i,\n",
    "    new_vocab_dict_i\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8033333333333333, 0.19666666666666666]\n",
      "13473.0\n",
      "15861.0\n",
      "71 1063 4481 4577\n",
      "Accuracy: 0.9825\n",
      "Confusion Matrix:\n",
      " [[318   2]\n",
      " [  5  75]]\n",
      "Precision: 0.97\n",
      "Recall: 0.94\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Supervised model evaluation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test model on test data"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:53.395479Z",
     "start_time": "2025-04-10T14:56:51.796054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluate our model on the test set\n",
    "_, _ = validation_eval(test_df['class'].values, class_0_c_new_i,\n",
    "                class_1_c_new_i, test_df, new_vocab_list_i, new_vocab_dict_i)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133 1508 10716 10968\n",
      "Accuracy: 0.977\n",
      "Confusion Matrix:\n",
      " [[785  15]\n",
      " [  8 192]]\n",
      "Precision: 0.93\n",
      "Recall: 0.96\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model's Representation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:53.450389Z",
     "start_time": "2025-04-10T14:56:53.435370Z"
    }
   },
   "source": [
    "# Compare model confidence before and after semi-supervised training\n",
    "def compare_model_confidence(c0_posterior_before, c1_posterior_before, c0_posterior_after, c1_posterior_after):\n",
    "    # Calculate confidence ratios before and after training\n",
    "    confidence_ratios_before = []\n",
    "    confidence_ratios_after = []\n",
    "\n",
    "    for (c0, c1) in zip(c0_posterior_before, c1_posterior_before):\n",
    "        c0_c1 = c0 / c1\n",
    "        c1_c0 = c1 / c0\n",
    "        confidence_ratios_before.append([c0_c1, c1_c0])\n",
    "\n",
    "    for (c0, c1) in zip(c0_posterior_after, c1_posterior_after):\n",
    "        c0_c1 = c0 / c1\n",
    "        c1_c0 = c1 / c0\n",
    "        confidence_ratios_after.append([c0_c1, c1_c0])\n",
    "\n",
    "\n",
    "    # Calculate and print average confidence for each class\n",
    "    avg_confidence_before_c0_c1 = np.mean([ratio[0] for ratio in confidence_ratios_before])\n",
    "    avg_confidence_before_c1_c0 = np.mean([ratio[1] for ratio in confidence_ratios_before])\n",
    "    avg_confidence_after_c0_c1 = np.mean([ratio[0] for ratio in confidence_ratios_after])\n",
    "    avg_confidence_after_c1_c0 = np.mean([ratio[1] for ratio in confidence_ratios_after])\n",
    "\n",
    "    print(f\"Average confidence before training (c0/c1): {avg_confidence_before_c0_c1:.4e}\")\n",
    "    print(f\"Average confidence before training (c1/c0): {avg_confidence_before_c1_c0:.4e}\")\n",
    "    print(f\"Average confidence after training (c0/c1): {avg_confidence_after_c0_c1:.4e}\")\n",
    "    print(f\"Average confidence after training (c1/c0): {avg_confidence_after_c1_c0:.4e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Compare most predictive words before and after our active learning\n",
    "def compare_most_predictive_words(vocab_list_before, class_0_c_before, class_1_c_before,\n",
    "                                   vocab_list_after, class_0_c_after, class_1_c_after, n=10):\n",
    "    # Convert lists to NumPy arrays\n",
    "    class_0_c_before = np.array(class_0_c_before)\n",
    "    class_1_c_before = np.array(class_1_c_before)\n",
    "    class_0_c_after = np.array(class_0_c_after)\n",
    "    class_1_c_after = np.array(class_1_c_after)\n",
    "\n",
    "    # Find the most predictive words before training (ratio of likelihoods)\n",
    "    most_predictive_words_c0_before = [(vocab_list_before[i], (class_0_c_before[i] / class_1_c_before[i])) for i in np.argsort(class_0_c_before / class_1_c_before)[-n:][::-1]]\n",
    "    most_predictive_words_c1_before = [(vocab_list_before[i], (class_1_c_before[i] / class_0_c_before[i])) for i in np.argsort(class_1_c_before / class_0_c_before)[-n:][::-1]]\n",
    "\n",
    "    # Find the most predictive words after training\n",
    "    most_predictive_words_c0_after = [(vocab_list_after[i], (class_0_c_after[i] / class_1_c_after[i])) for i in np.argsort(class_0_c_after / class_1_c_after)[-n:][::-1]]\n",
    "    most_predictive_words_c1_after = [(vocab_list_after[i], (class_1_c_after[i] / class_0_c_after[i])) for i in np.argsort(class_1_c_after / class_0_c_after)[-n:][::-1]]\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Most predictive words for class 0 before training with ratios:\")\n",
    "    for word, ratio in most_predictive_words_c0_before:\n",
    "        print(f\"{word}: {ratio}\")\n",
    "\n",
    "    print(\"\\nMost predictive words for class 0 after training with ratios:\")\n",
    "    for word, ratio in most_predictive_words_c0_after:\n",
    "        print(f\"{word}: {ratio}\")\n",
    "\n",
    "    print(\"\\nMost predictive words for class 1 before training with ratios:\")\n",
    "    for word, ratio in most_predictive_words_c1_before:\n",
    "        print(f\"{word}: {ratio}\")\n",
    "\n",
    "    print(\"\\nMost predictive words for class 1 after training with ratios:\")\n",
    "    for word, ratio in most_predictive_words_c1_after:\n",
    "        print(f\"{word}: {ratio}\")\n",
    "\n",
    "\n",
    "\n",
    "# Call the functions\n",
    "compare_model_confidence(c0_posterior, c1_posterior, c0_posteriors_i, c1_posteriors_i)\n",
    "compare_most_predictive_words(vocab_list, class_0_c, class_1_c, new_vocab_list_i, class_0_c_new_i, class_1_c_new_i)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average confidence before training (c0/c1): 9.1350e+34\n",
      "Average confidence before training (c1/c0): 5.9150e+17\n",
      "Average confidence after training (c0/c1): 3.2162e+17\n",
      "Average confidence after training (c1/c0): 7.7935e+14\n",
      "Most predictive words for class 0 before training with ratios:\n",
      ";: 60.49921647638236\n",
      "...: 57.4957092754272\n",
      "gt: 54.06312961719275\n",
      "lt: 53.548242668457576\n",
      ":): 47.88448623237072\n",
      "ü: 31.922990821580477\n",
      "lor: 28.833669129169465\n",
      "hope: 24.714573539288114\n",
      "ok: 24.714573539288114\n",
      "d: 21.110364898141928\n",
      "\n",
      "Most predictive words for class 0 after training with ratios:\n",
      "gt: 49.38611165142968\n",
      "lt: 48.86072748492511\n",
      ":): 36.77689165531998\n",
      "...: 28.68597549114958\n",
      "ü: 27.845360824742265\n",
      ";: 27.582668741489982\n",
      "lor: 23.116903326201125\n",
      "ok: 17.863061661155417\n",
      "hope: 17.863061661155417\n",
      "d: 17.863061661155417\n",
      "\n",
      "Most predictive words for class 1 before training with ratios:\n",
      "prize: 99.05086956521738\n",
      "tone: 64.09173913043477\n",
      "£: 49.71965217391304\n",
      "select: 46.61217391304348\n",
      "claim: 45.96478260869565\n",
      "paytm: 36.901304347826084\n",
      "code: 34.95913043478261\n",
      "award: 32.04586956521739\n",
      "won: 31.074782608695653\n",
      "18: 29.13260869565217\n",
      "\n",
      "Most predictive words for class 1 after training with ratios:\n",
      "prize: 87.55497963717141\n",
      "tone: 51.39096630877453\n",
      "claim: 36.7984697025793\n",
      "select: 36.16401332839689\n",
      "paytm: 36.16401332839689\n",
      "£: 33.62618783166729\n",
      "ringtone: 28.55053683820807\n",
      "code: 26.647167715660867\n",
      "won: 26.647167715660867\n",
      "18: 24.743798593113663\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
